{"version":3,"file":"cv-worker.js","mappings":";;;;;;;;;;;;;;;;;;;;AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;;;;;;;;AClFA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;AC/PA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;ACvBA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;ACPA;;;;;ACAA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;;ACNA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA","sources":["webpack://eyezen-chrome-extension/./types/index.ts","webpack://eyezen-chrome-extension/./types/mediapipe.ts","webpack://eyezen-chrome-extension/webpack/bootstrap","webpack://eyezen-chrome-extension/webpack/runtime/define property getters","webpack://eyezen-chrome-extension/webpack/runtime/hasOwnProperty shorthand","webpack://eyezen-chrome-extension/webpack/runtime/make namespace object","webpack://eyezen-chrome-extension/./core/cv-worker/worker.ts"],"sourcesContent":["// Core types for EyeZen Chrome Extension\nexport var PostureStatus;\n(function (PostureStatus) {\n    PostureStatus[\"GOOD\"] = \"good\";\n    PostureStatus[\"FORWARD\"] = \"forward\";\n    PostureStatus[\"TILTED\"] = \"tilted\";\n    PostureStatus[\"TOO_CLOSE\"] = \"too_close\";\n    PostureStatus[\"TOO_FAR\"] = \"too_far\";\n})(PostureStatus || (PostureStatus = {}));\n// User status and scoring\nexport var UserStatus;\n(function (UserStatus) {\n    UserStatus[\"GOOD\"] = \"good\";\n    UserStatus[\"TIRED\"] = \"tired\";\n    UserStatus[\"CRITICAL\"] = \"critical\";\n})(UserStatus || (UserStatus = {}));\nexport var BreakType;\n(function (BreakType) {\n    BreakType[\"MICRO\"] = \"micro\";\n    BreakType[\"SHORT\"] = \"short\";\n    BreakType[\"LONG\"] = \"long\"; // 15 minutes\n})(BreakType || (BreakType = {}));\nexport var MassagePointType;\n(function (MassagePointType) {\n    MassagePointType[\"ZAN_ZHU\"] = \"zan_zhu\";\n    MassagePointType[\"SI_BAI\"] = \"si_bai\";\n    MassagePointType[\"JING_MING\"] = \"jing_ming\"; // 睛明\n})(MassagePointType || (MassagePointType = {}));\n// Error types\nexport class EyeZenError extends Error {\n    constructor(message, code, severity = 'medium') {\n        super(message);\n        this.code = code;\n        this.severity = severity;\n        this.name = 'EyeZenError';\n    }\n}\n// Constants\nexport const DEFAULT_SETTINGS = {\n    cameraEnabled: true,\n    detectionSensitivity: 'medium',\n    fatigueThreshold: 70,\n    reminderEnabled: true,\n    reminderInterval: 20,\n    breakDuration: 20,\n    dataRetention: 30,\n    metricsOnly: false,\n    language: 'en',\n    theme: 'auto',\n    notifications: true,\n    sounds: true,\n    dailyBreakGoal: 8,\n    eyeScoreGoal: 80\n};\nexport const MASSAGE_POINTS = {\n    [MassagePointType.ZAN_ZHU]: {\n        name: 'Zan Zhu',\n        chineseName: '攒竹',\n        position: { x: 0.3, y: 0.25 },\n        description: 'Inner end of eyebrow',\n        benefits: ['Relieves eye strain', 'Reduces headaches', 'Improves focus'],\n        duration: 30\n    },\n    [MassagePointType.SI_BAI]: {\n        name: 'Si Bai',\n        chineseName: '四白',\n        position: { x: 0.35, y: 0.45 },\n        description: 'Below the center of the eye',\n        benefits: ['Brightens eyes', 'Reduces dark circles', 'Improves circulation'],\n        duration: 30\n    },\n    [MassagePointType.JING_MING]: {\n        name: 'Jing Ming',\n        chineseName: '睛明',\n        position: { x: 0.25, y: 0.35 },\n        description: 'Inner corner of the eye',\n        benefits: ['Clears vision', 'Reduces eye fatigue', 'Calms the mind'],\n        duration: 30\n    }\n};\n// Export all types\n// Note: Chrome types will be available via @types/chrome package\n// Additional type exports can be added here as needed\n","// MediaPipe Face Landmarker types and utilities\nimport { PostureStatus } from './index';\n// Eye landmark indices for MediaPipe Face Landmarker\nexport const EYE_LANDMARKS = {\n    LEFT_EYE: {\n        OUTER_CORNER: 33,\n        INNER_CORNER: 133,\n        TOP_LID: [159, 158, 157, 173],\n        BOTTOM_LID: [144, 145, 153, 154],\n        PUPIL: 468\n    },\n    RIGHT_EYE: {\n        OUTER_CORNER: 362,\n        INNER_CORNER: 263,\n        TOP_LID: [386, 385, 384, 398],\n        BOTTOM_LID: [373, 374, 380, 381],\n        PUPIL: 473\n    }\n};\n// Face outline landmarks\nexport const FACE_LANDMARKS = {\n    FACE_OVAL: [10, 338, 297, 332, 284, 251, 389, 356, 454, 323, 361, 288, 397, 365, 379, 378, 400, 377, 152, 148, 176, 149, 150, 136, 172, 58, 132, 93, 234, 127, 162, 21, 54, 103, 67, 109],\n    NOSE_BRIDGE: [6, 168, 8, 9, 10, 151],\n    NOSE_TIP: [1, 2, 5, 4, 19, 94, 125],\n    MOUTH_OUTER: [61, 84, 17, 314, 405, 320, 307, 375, 321, 308, 324, 318],\n    MOUTH_INNER: [78, 95, 88, 178, 87, 14, 317, 402, 318, 324, 308]\n};\n// Eye Aspect Ratio (EAR) calculation utilities\nexport class EARCalculator {\n    /**\n     * Calculate Eye Aspect Ratio for fatigue detection\n     * EAR = (|p2-p6| + |p3-p5|) / (2 * |p1-p4|)\n     */\n    static calculateEAR(eyeLandmarks) {\n        if (eyeLandmarks.length < 6) {\n            throw new Error('Insufficient eye landmarks for EAR calculation');\n        }\n        // Extract key points (assuming 6-point eye model)\n        const [p1, p2, p3, p4, p5, p6] = eyeLandmarks;\n        // Calculate distances\n        const vertical1 = this.euclideanDistance(p2, p6);\n        const vertical2 = this.euclideanDistance(p3, p5);\n        const horizontal = this.euclideanDistance(p1, p4);\n        // Calculate EAR\n        const ear = (vertical1 + vertical2) / (2.0 * horizontal);\n        return ear;\n    }\n    /**\n     * Calculate average EAR for both eyes\n     */\n    static calculateAverageEAR(leftEyeLandmarks, rightEyeLandmarks) {\n        const leftEAR = this.calculateEAR(leftEyeLandmarks);\n        const rightEAR = this.calculateEAR(rightEyeLandmarks);\n        return (leftEAR + rightEAR) / 2.0;\n    }\n    static euclideanDistance(point1, point2) {\n        const dx = point1.x - point2.x;\n        const dy = point1.y - point2.y;\n        const dz = point1.z - point2.z;\n        return Math.sqrt(dx * dx + dy * dy + dz * dz);\n    }\n}\n// PERCLOS (Percentage of Eye Closure) calculation\nexport class PERCLOSCalculator {\n    constructor(windowSize = 30, closureThreshold = 0.2) {\n        this.earHistory = [];\n        this.windowSize = windowSize;\n        this.closureThreshold = closureThreshold;\n    }\n    /**\n     * Add new EAR value and calculate PERCLOS\n     */\n    addEARValue(ear) {\n        this.earHistory.push(ear);\n        // Maintain sliding window\n        if (this.earHistory.length > this.windowSize) {\n            this.earHistory.shift();\n        }\n        return this.calculatePERCLOS();\n    }\n    /**\n     * Calculate PERCLOS as percentage of time eyes are closed\n     */\n    calculatePERCLOS() {\n        if (this.earHistory.length === 0)\n            return 0;\n        const closedFrames = this.earHistory.filter(ear => ear < this.closureThreshold).length;\n        return (closedFrames / this.earHistory.length) * 100;\n    }\n    /**\n     * Reset the calculation window\n     */\n    reset() {\n        this.earHistory = [];\n    }\n    /**\n     * Get current window size\n     */\n    getWindowSize() {\n        return this.earHistory.length;\n    }\n}\n// Blink detection utilities\nexport class BlinkDetector {\n    constructor(blinkThreshold = 0.2, minBlinkDuration = 100, // ms\n    maxBlinkDuration = 400 // ms\n    ) {\n        this.earHistory = [];\n        this.blinkTimestamps = [];\n        this.startTime = 0;\n        this.isInBlink = false;\n        this.blinkStartTime = 0;\n        this.blinkThreshold = blinkThreshold;\n        this.minBlinkDuration = minBlinkDuration;\n        this.maxBlinkDuration = maxBlinkDuration;\n        this.startTime = Date.now();\n    }\n    /**\n     * Process new EAR value and detect blinks\n     */\n    processEAR(ear, timestamp) {\n        this.earHistory.push({ value: ear, timestamp });\n        // Keep only recent history (last 2 seconds)\n        const cutoffTime = timestamp - 2000;\n        this.earHistory = this.earHistory.filter(entry => entry.timestamp > cutoffTime);\n        // Clean old blink timestamps (keep only last minute)\n        const oneMinuteAgo = timestamp - 60000;\n        this.blinkTimestamps = this.blinkTimestamps.filter(t => t > oneMinuteAgo);\n        // Detect blink state changes\n        const blinkDetected = this.detectBlinkTransition(ear, timestamp);\n        if (blinkDetected) {\n            this.blinkTimestamps.push(timestamp);\n            console.log('🔍 Blink detected! Total blinks in last minute:', this.blinkTimestamps.length);\n            return true;\n        }\n        return false;\n    }\n    /**\n     * Get blink rate (blinks per minute)\n     */\n    getBlinkRate(timeWindowMs = 60000) {\n        const currentTime = Date.now();\n        const windowStart = currentTime - timeWindowMs;\n        // Count blinks in the time window\n        const blinksInWindow = this.blinkTimestamps.filter(t => t > windowStart).length;\n        // Calculate elapsed time since start or window size, whichever is smaller\n        const elapsedTime = Math.min(timeWindowMs, currentTime - this.startTime);\n        if (elapsedTime < 1000)\n            return 0; // Need at least 1 second of data\n        // Convert to blinks per minute\n        const blinkRate = (blinksInWindow / elapsedTime) * 60000;\n        console.log(`📊 Blink rate calculation: ${blinksInWindow} blinks in ${(elapsedTime / 1000).toFixed(1)}s = ${blinkRate.toFixed(1)} bpm`);\n        return blinkRate;\n    }\n    /**\n     * Reset blink counter\n     */\n    reset() {\n        this.earHistory = [];\n        this.blinkTimestamps = [];\n        this.startTime = Date.now();\n        this.isInBlink = false;\n        this.blinkStartTime = 0;\n    }\n    detectBlinkTransition(ear, timestamp) {\n        if (!this.isInBlink && ear <= this.blinkThreshold) {\n            // Start of blink\n            this.isInBlink = true;\n            this.blinkStartTime = timestamp;\n            return false; // Don't count as complete blink yet\n        }\n        else if (this.isInBlink && ear > this.blinkThreshold) {\n            // End of blink\n            this.isInBlink = false;\n            const blinkDuration = timestamp - this.blinkStartTime;\n            // Validate blink duration\n            if (blinkDuration >= this.minBlinkDuration && blinkDuration <= this.maxBlinkDuration) {\n                return true; // Valid blink detected\n            }\n        }\n        return false;\n    }\n}\n// Head pose estimation\nexport class HeadPoseEstimator {\n    /**\n     * Estimate head pose from face landmarks\n     */\n    static estimatePose(faceLandmarks) {\n        // Use key facial landmarks for pose estimation\n        const noseTip = faceLandmarks[1];\n        const leftEyeCorner = faceLandmarks[33];\n        const rightEyeCorner = faceLandmarks[263];\n        const leftMouthCorner = faceLandmarks[61];\n        const rightMouthCorner = faceLandmarks[291];\n        // Calculate yaw (left-right rotation)\n        const eyeDistance = Math.abs(leftEyeCorner.x - rightEyeCorner.x);\n        const noseCenterX = (leftEyeCorner.x + rightEyeCorner.x) / 2;\n        const yaw = (noseTip.x - noseCenterX) / eyeDistance;\n        // Calculate pitch (up-down rotation)\n        const eyeCenterY = (leftEyeCorner.y + rightEyeCorner.y) / 2;\n        const mouthCenterY = (leftMouthCorner.y + rightMouthCorner.y) / 2;\n        const faceHeight = Math.abs(mouthCenterY - eyeCenterY);\n        const pitch = (noseTip.y - eyeCenterY) / faceHeight;\n        // Calculate roll (tilt rotation)\n        const eyeSlope = (rightEyeCorner.y - leftEyeCorner.y) / (rightEyeCorner.x - leftEyeCorner.x);\n        const roll = Math.atan(eyeSlope);\n        return {\n            pitch: pitch * 180 / Math.PI, // Convert to degrees\n            yaw: yaw * 180 / Math.PI,\n            roll: roll * 180 / Math.PI\n        };\n    }\n    /**\n     * Classify posture based on head pose\n     */\n    static classifyPosture(pose) {\n        const { pitch, yaw, roll } = pose;\n        // Define thresholds\n        const PITCH_THRESHOLD = 15;\n        const YAW_THRESHOLD = 20;\n        const ROLL_THRESHOLD = 15;\n        if (Math.abs(pitch) > PITCH_THRESHOLD) {\n            return pitch > 0 ?\n                PostureStatus.FORWARD :\n                PostureStatus.GOOD;\n        }\n        if (Math.abs(yaw) > YAW_THRESHOLD || Math.abs(roll) > ROLL_THRESHOLD) {\n            return PostureStatus.TILTED;\n        }\n        return PostureStatus.GOOD;\n    }\n}\n// MediaPipe initialization utilities\nexport class MediaPipeInitializer {\n    static async loadModel(modelPath) {\n        // This will be implemented with actual MediaPipe Face Landmarker\n        // when the worker is created\n        throw new Error('MediaPipe model loading not implemented yet');\n    }\n    static createConfig(options = {}) {\n        return {\n            baseOptions: {\n                modelAssetPath: options.baseOptions?.modelAssetPath || '/assets/wasm/face_landmarker.task',\n                delegate: options.baseOptions?.delegate || 'CPU'\n            },\n            runningMode: options.runningMode || 'VIDEO',\n            numFaces: options.numFaces || 1,\n            minFaceDetectionConfidence: options.minFaceDetectionConfidence || 0.5,\n            minFacePresenceConfidence: options.minFacePresenceConfidence || 0.5,\n            minTrackingConfidence: options.minTrackingConfidence || 0.5,\n            outputFaceBlendshapes: options.outputFaceBlendshapes || false,\n            outputFacialTransformationMatrixes: options.outputFacialTransformationMatrixes || false\n        };\n    }\n}\n","// The module cache\nvar __webpack_module_cache__ = {};\n\n// The require function\nfunction __webpack_require__(moduleId) {\n\t// Check if module is in cache\n\tvar cachedModule = __webpack_module_cache__[moduleId];\n\tif (cachedModule !== undefined) {\n\t\treturn cachedModule.exports;\n\t}\n\t// Create a new module (and put it into the cache)\n\tvar module = __webpack_module_cache__[moduleId] = {\n\t\t// no module.id needed\n\t\t// no module.loaded needed\n\t\texports: {}\n\t};\n\n\t// Execute the module function\n\t__webpack_modules__[moduleId](module, module.exports, __webpack_require__);\n\n\t// Return the exports of the module\n\treturn module.exports;\n}\n\n","// define getter functions for harmony exports\n__webpack_require__.d = (exports, definition) => {\n\tfor(var key in definition) {\n\t\tif(__webpack_require__.o(definition, key) && !__webpack_require__.o(exports, key)) {\n\t\t\tObject.defineProperty(exports, key, { enumerable: true, get: definition[key] });\n\t\t}\n\t}\n};","__webpack_require__.o = (obj, prop) => (Object.prototype.hasOwnProperty.call(obj, prop))","// define __esModule on exports\n__webpack_require__.r = (exports) => {\n\tif(typeof Symbol !== 'undefined' && Symbol.toStringTag) {\n\t\tObject.defineProperty(exports, Symbol.toStringTag, { value: 'Module' });\n\t}\n\tObject.defineProperty(exports, '__esModule', { value: true });\n};","/**\n * CV Worker for EyeZen Chrome Extension\n * Uses MediaPipe Face Landmarker (WASM) to compute EAR/PERCLOS from camera frames\n * Ensures proper frame cleanup and memory management\n */\nimport { EARCalculator, PERCLOSCalculator, BlinkDetector, HeadPoseEstimator } from '../../types/mediapipe';\nimport { PostureStatus } from '../../types/index';\nclass CVWorker {\n    constructor() {\n        this.faceLandmarker = null;\n        this.isProcessing = false;\n        this.lastFrameTime = 0;\n        this.frameCount = 0;\n        this.targetFPS = 15; // Limit processing to 15 FPS for performance\n        this.frameInterval = 1000 / this.targetFPS;\n        this.perclosCalculator = new PERCLOSCalculator(30, 0.2); // 30 frame window, 0.2 closure threshold\n        this.blinkDetector = new BlinkDetector(0.2, 100, 400); // threshold, min/max blink duration\n        // Listen for messages from main thread\n        self.addEventListener('message', this.handleMessage.bind(this));\n    }\n    /**\n     * Handle messages from the main thread\n     */\n    async handleMessage(event) {\n        const { type, data } = event.data;\n        const messageTimestamp = new Date().toISOString();\n        console.log(`📨 [${messageTimestamp}] CV Worker: Received ${type} message`, data ? 'with data' : 'without data');\n        // Only log non-process messages to reduce console noise\n        if (type !== 'process') {\n            console.log('🔧 CV Worker received message:', type, data ? 'with data' : 'no data');\n        }\n        try {\n            switch (type) {\n                case 'init':\n                    await this.initialize(data);\n                    break;\n                case 'start':\n                    console.log('🚀 CV Worker: Starting processing');\n                    this.startProcessing();\n                    break;\n                case 'process':\n                    if (data) {\n                        const timestamp = new Date().toISOString();\n                        console.log(`🎯 [${timestamp}] CV Worker: Processing frame message received`);\n                        await this.processFrame(data);\n                    }\n                    else {\n                        console.warn('⚠️ CV Worker: Process message received without data');\n                    }\n                    break;\n                case 'stop':\n                    this.stopProcessing();\n                    break;\n                case 'cleanup':\n                    await this.cleanup();\n                    break;\n                default:\n                    this.postError(`Unknown message type: ${type}`);\n            }\n        }\n        catch (error) {\n            const errorMessage = error instanceof Error ? error.message : String(error);\n            this.postError(`Error handling ${type}: ${errorMessage}`);\n        }\n    }\n    /**\n     * Initialize MediaPipe Face Landmarker\n     */\n    async initialize(config) {\n        try {\n            console.log('🚀 CV Worker: Initializing MediaPipe Face Landmarker...');\n            // Load MediaPipe scripts dynamically\n            await this.loadMediaPipeScripts();\n            // MediaPipe is now initialized through the worker loader\n            // Set a flag to indicate we're ready to process\n            this.faceLandmarker = { initialized: true };\n            console.log('✅ CV Worker: MediaPipe Face Landmarker initialized successfully');\n            this.postMessage({\n                type: 'ready',\n                data: { message: 'CV Worker initialized successfully' }\n            });\n        }\n        catch (error) {\n            const errorMessage = error instanceof Error ? error.message : String(error);\n            console.error('❌ CV Worker: Failed to initialize MediaPipe:', error);\n            this.postError(`Failed to initialize MediaPipe: ${errorMessage}`);\n        }\n    }\n    /**\n     * Load MediaPipe scripts dynamically in worker context\n     */\n    async loadMediaPipeScripts() {\n        return new Promise(async (resolve, reject) => {\n            try {\n                console.log('📦 CV Worker: Loading MediaPipe scripts...');\n                // Load MediaPipe worker loader from local assets\n                importScripts('./assets/mediapipe-worker-loader.js');\n                // Initialize MediaPipe using the worker loader\n                const { initializeMediaPipe, detectForVideo } = await globalThis.MediaPipeWorkerLoader.loadVisionTasks();\n                // Store the detection function globally for use in processFrame\n                globalThis.detectForVideo = detectForVideo;\n                console.log('✅ CV Worker: MediaPipe scripts loaded successfully');\n                resolve();\n            }\n            catch (error) {\n                console.error('❌ CV Worker: Failed to load MediaPipe scripts:', error);\n                reject(error);\n            }\n        });\n    }\n    /**\n     * Process a single video frame\n     */\n    async processFrame(frameData) {\n        if (!this.faceLandmarker || !this.isProcessing) {\n            console.log('🚫 CV Worker: Skipping frame - faceLandmarker:', !!this.faceLandmarker, 'isProcessing:', this.isProcessing);\n            return;\n        }\n        try {\n            const { imageData, timestamp } = frameData;\n            this.frameCount++;\n            const logTimestamp = new Date().toISOString();\n            console.log(`🔍 [${logTimestamp}] CV Worker: Processing detection frame ${this.frameCount} at timestamp ${timestamp}`);\n            console.log(`🔍 [${logTimestamp}] CV Worker: detectForVideo available?`, typeof globalThis.detectForVideo);\n            // Create HTMLCanvasElement from ImageData for MediaPipe\n            const canvas = new OffscreenCanvas(imageData.width, imageData.height);\n            const ctx = canvas.getContext('2d', { willReadFrequently: true });\n            ctx.putImageData(imageData, 0, 0);\n            // Check if detection function is available\n            if (typeof globalThis.detectForVideo !== 'function') {\n                console.error('❌ CV Worker: detectForVideo function not available in worker context');\n                return;\n            }\n            // Process frame with MediaPipe using the global detection function\n            console.log(`🎯 [${logTimestamp}] CV Worker: Calling detectForVideo with canvas:`, canvas.width, 'x', canvas.height);\n            const results = await globalThis.detectForVideo(canvas, timestamp);\n            console.log('🎯 CV Worker: detectForVideo returned:', results);\n            // Extract metrics if face is detected\n            if (results && results.faceLandmarks && results.faceLandmarks.length > 0) {\n                const metrics = this.extractMetrics(results.faceLandmarks[0], timestamp);\n                // Only log eye metrics occasionally to reduce console noise\n                if (this.frameCount % 150 === 0) { // Log every 10 seconds\n                    console.log('👁️ CV Worker: Eye metrics calculated:', {\n                        blinkRate: metrics.blinkRate,\n                        fatigueIndex: metrics.fatigueIndex,\n                        earValue: metrics.earValue,\n                        perclosValue: metrics.perclosValue\n                    });\n                }\n                this.postMessage({\n                    type: 'metrics',\n                    data: metrics\n                });\n            }\n            else {\n                if (this.frameCount % 300 === 0) { // Log every 20 seconds when no face detected\n                    console.log('👤 CV Worker: No face detected in frame');\n                }\n            }\n            // Clean up canvas immediately\n            ctx.clearRect(0, 0, canvas.width, canvas.height);\n            this.lastFrameTime = performance.now();\n        }\n        catch (error) {\n            const errorMessage = error instanceof Error ? error.message : String(error);\n            console.error('❌ CV Worker: Frame processing error:', error);\n            this.postError(`Frame processing error: ${errorMessage}`);\n        }\n    }\n    /**\n     * Extract eye metrics from face landmarks\n     */\n    extractMetrics(faceLandmarks, timestamp) {\n        try {\n            // Extract eye landmarks (MediaPipe Face Landmarker indices)\n            const leftEyeLandmarks = this.getEyeLandmarks(faceLandmarks, 'left');\n            const rightEyeLandmarks = this.getEyeLandmarks(faceLandmarks, 'right');\n            // Calculate EAR (Eye Aspect Ratio)\n            const earValue = EARCalculator.calculateAverageEAR(leftEyeLandmarks, rightEyeLandmarks);\n            console.log('👁️ Raw EAR Value:', earValue.toFixed(3));\n            // Calculate PERCLOS (Percentage of Eye Closure)\n            const perclosValue = this.perclosCalculator.addEARValue(earValue);\n            console.log('😴 PERCLOS Value:', perclosValue.toFixed(3), '%');\n            // Detect blinks\n            const isBlinking = this.blinkDetector.processEAR(earValue, timestamp);\n            const blinkRate = this.blinkDetector.getBlinkRate();\n            console.log('👀 Blink Detection - Is Blinking:', isBlinking, 'Rate:', blinkRate.toFixed(1), 'bpm');\n            // Estimate head pose\n            const headPose = HeadPoseEstimator.estimatePose(faceLandmarks);\n            const posture = HeadPoseEstimator.classifyPosture(headPose);\n            console.log('🧍 Head Pose:', headPose, 'Posture:', posture);\n            // Calculate fatigue index (0-100 scale)\n            const fatigueIndex = this.calculateFatigueIndex({\n                ear: earValue,\n                perclos: perclosValue,\n                blinkRate,\n                posture\n            });\n            return {\n                blinkRate,\n                fatigueIndex,\n                posture,\n                earValue,\n                perclosValue,\n                timestamp\n            };\n        }\n        catch (error) {\n            const errorMessage = error instanceof Error ? error.message : String(error);\n            throw new Error(`Metrics extraction failed: ${errorMessage}`);\n        }\n    }\n    /**\n     * Extract eye landmarks for a specific eye\n     */\n    getEyeLandmarks(faceLandmarks, eye) {\n        // MediaPipe Face Landmarker 6-point eye model for EAR calculation\n        // Format: [outer_corner, top_1, top_2, inner_corner, bottom_2, bottom_1]\n        const leftEyeIndices = [33, 159, 158, 133, 153, 144]; // Left eye 6 points\n        const rightEyeIndices = [362, 386, 385, 263, 374, 373]; // Right eye 6 points\n        const indices = eye === 'left' ? leftEyeIndices : rightEyeIndices;\n        const landmarks = indices.map(index => faceLandmarks[index]).filter(Boolean);\n        console.log(`👁️ ${eye} eye landmarks (${landmarks.length} points):`, landmarks.map((l, i) => `${indices[i]}: (${l.x.toFixed(3)}, ${l.y.toFixed(3)})`));\n        return landmarks;\n    }\n    /**\n     * Calculate fatigue index based on multiple metrics\n     */\n    calculateFatigueIndex(metrics) {\n        const { ear, perclos, blinkRate, posture } = metrics;\n        console.log('🔍 Fatigue Calculation Input:', {\n            ear: ear.toFixed(3),\n            perclos: perclos.toFixed(3),\n            blinkRate: blinkRate.toFixed(1),\n            posture\n        });\n        // Normalize metrics to 0-100 scale (lower fatigue score = better health)\n        let fatigueScore = 0;\n        // EAR contribution (lower EAR = more fatigue) - made less punitive\n        const normalEAR = 0.3; // Typical EAR for alert state\n        const earDeviation = Math.max(0, normalEAR - ear);\n        const earScore = Math.min(25, (earDeviation / normalEAR) * 25); // Reduced from 40 to 25\n        fatigueScore += earScore;\n        console.log('📊 EAR Score:', earScore.toFixed(1), '(EAR:', ear.toFixed(3), 'vs normal:', normalEAR, ')');\n        // PERCLOS contribution (higher PERCLOS = more fatigue) - made less punitive\n        const perclosScore = Math.min(25, perclos * 100); // Reduced impact and cap at 25\n        fatigueScore += perclosScore;\n        console.log('📊 PERCLOS Score:', perclosScore.toFixed(1), '(PERCLOS:', perclos.toFixed(3), '%)');\n        // Blink rate contribution - made less punitive\n        const normalBlinkRate = 17.5; // Optimal blinks per minute (middle of 15-20 range)\n        const blinkRateDeviation = Math.abs(blinkRate - normalBlinkRate);\n        const blinkScore = Math.min(15, blinkRateDeviation / 3); // Reduced impact\n        fatigueScore += blinkScore;\n        console.log('📊 Blink Score:', blinkScore.toFixed(1), '(Rate:', blinkRate.toFixed(1), 'vs normal:', normalBlinkRate, ')');\n        // Posture contribution - made less punitive\n        const postureScore = posture === PostureStatus.GOOD ? 0 :\n            posture === PostureStatus.FORWARD ? 5 : // Reduced from 8\n                posture === PostureStatus.TILTED ? 3 : // Reduced from 5\n                    7; // Reduced from 10\n        fatigueScore += postureScore;\n        console.log('📊 Posture Score:', postureScore, '(Status:', posture, ')');\n        // Cap the final score at a more reasonable level\n        const finalScore = Math.min(70, Math.max(0, fatigueScore)); // Reduced max from 100 to 70\n        console.log('🎯 Final Fatigue Index:', finalScore.toFixed(1), '/ 70 (capped)');\n        return finalScore;\n    }\n    /**\n     * Start processing frames\n     */\n    startProcessing() {\n        this.isProcessing = true;\n        this.frameCount = 0;\n        this.lastFrameTime = 0;\n        this.perclosCalculator.reset();\n        this.blinkDetector.reset();\n    }\n    /**\n     * Stop processing frames\n     */\n    stopProcessing() {\n        this.isProcessing = false;\n        this.postMessage({\n            type: 'stopped',\n            data: { frameCount: this.frameCount }\n        });\n    }\n    /**\n     * Clean up resources\n     */\n    async cleanup() {\n        this.isProcessing = false;\n        if (this.faceLandmarker) {\n            this.faceLandmarker.close();\n            this.faceLandmarker = null;\n        }\n        this.perclosCalculator.reset();\n        this.blinkDetector.reset();\n        this.postMessage({\n            type: 'stopped',\n            data: { message: 'CV Worker cleaned up' }\n        });\n    }\n    /**\n     * Post message to main thread\n     */\n    postMessage(response) {\n        self.postMessage(response);\n    }\n    /**\n     * Post error to main thread\n     */\n    postError(message) {\n        this.postMessage({\n            type: 'error',\n            data: { error: message }\n        });\n    }\n}\n// Initialize worker\nconst cvWorker = new CVWorker();\n// Export for TypeScript\nexport default cvWorker;\n"],"names":[],"sourceRoot":""}